{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9895b8e",
   "metadata": {},
   "source": [
    "# Cancer Patient Survival Prediction using Neural Networks\n",
    "\n",
    "This notebook implements a feedforward neural network to predict patient survival status based on clinical features from the China Cancer Patient Records dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c003b8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.19.0\n",
      "GPU available: []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Advanced libraries\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE\n",
    "    from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "    import xgboost as xgb\n",
    "    print(\"✓ Advanced libraries loaded successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"⚠️ Some advanced libraries not available: {e}\")\n",
    "    print(\"Install with: pip install imbalanced-learn xgboost\")\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPU available:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe39a4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (10000, 20)\n",
      "Target distribution:\n",
      "SurvivalStatus\n",
      "Alive       7790\n",
      "Deceased    2210\n",
      "Name: count, dtype: int64\n",
      "Target distribution (%):\n",
      "SurvivalStatus\n",
      "Alive       77.9\n",
      "Deceased    22.1\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data_path = \"/Users/f/.cache/kagglehub/datasets/ak0212/china-cancer-patient-records/versions/1/china_cancer_patients_synthetic.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Target distribution:\\n{df['SurvivalStatus'].value_counts()}\")\n",
    "print(f\"Target distribution (%):\\n{df['SurvivalStatus'].value_counts(normalize=True) * 100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b38a284",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa5a50e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      "PatientID                  0\n",
      "Gender                     0\n",
      "Age                        0\n",
      "Province                   0\n",
      "Ethnicity                  0\n",
      "TumorType                  0\n",
      "CancerStage                0\n",
      "DiagnosisDate              0\n",
      "TumorSize                  0\n",
      "Metastasis                 0\n",
      "TreatmentType              0\n",
      "SurgeryDate             5673\n",
      "ChemotherapySessions       0\n",
      "RadiationSessions          0\n",
      "SurvivalStatus             0\n",
      "FollowUpMonths             0\n",
      "SmokingStatus              0\n",
      "AlcoholUse              5921\n",
      "GeneticMutation         7200\n",
      "Comorbidities           3715\n",
      "dtype: int64\n",
      "\n",
      "Missing values percentage:\n",
      "PatientID                0.00\n",
      "Gender                   0.00\n",
      "Age                      0.00\n",
      "Province                 0.00\n",
      "Ethnicity                0.00\n",
      "TumorType                0.00\n",
      "CancerStage              0.00\n",
      "DiagnosisDate            0.00\n",
      "TumorSize                0.00\n",
      "Metastasis               0.00\n",
      "TreatmentType            0.00\n",
      "SurgeryDate             56.73\n",
      "ChemotherapySessions     0.00\n",
      "RadiationSessions        0.00\n",
      "SurvivalStatus           0.00\n",
      "FollowUpMonths           0.00\n",
      "SmokingStatus            0.00\n",
      "AlcoholUse              59.21\n",
      "GeneticMutation         72.00\n",
      "Comorbidities           37.15\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Create a copy for preprocessing\n",
    "data = df.copy()\n",
    "\n",
    "# Check missing values\n",
    "print(\"Missing values per column:\")\n",
    "print(data.isnull().sum())\n",
    "print(\"\\nMissing values percentage:\")\n",
    "print((data.isnull().sum() / len(data)) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43c8a85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (10000, 16)\n",
      "Target vector shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Select features for the model (excluding ID, dates, and target)\n",
    "feature_columns = ['Gender', 'Age', 'Province', 'Ethnicity', 'TumorType', 'CancerStage', \n",
    "                  'TumorSize', 'Metastasis', 'TreatmentType', 'ChemotherapySessions', \n",
    "                  'RadiationSessions', 'FollowUpMonths', 'SmokingStatus', 'AlcoholUse', \n",
    "                  'GeneticMutation', 'Comorbidities']\n",
    "\n",
    "# Create feature dataframe\n",
    "X = data[feature_columns].copy()\n",
    "y = data['SurvivalStatus'].copy()\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Target vector shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccc16c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target encoding: {'Alive': np.int64(0), 'Deceased': np.int64(1)}\n",
      "Gender: 3 unique values\n",
      "Province: 13 unique values\n",
      "Ethnicity: 6 unique values\n",
      "TumorType: 6 unique values\n",
      "CancerStage: 4 unique values\n",
      "Metastasis: 2 unique values\n",
      "TreatmentType: 5 unique values\n",
      "SmokingStatus: 3 unique values\n",
      "AlcoholUse: 3 unique values\n",
      "GeneticMutation: 4 unique values\n",
      "Comorbidities: 10 unique values\n",
      "\n",
      "Processed feature matrix shape: (10000, 16)\n",
      "Missing values after preprocessing: 0\n"
     ]
    }
   ],
   "source": [
    "# Handle missing values and encode categorical variables\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize label encoders\n",
    "label_encoders = {}\n",
    "\n",
    "# Encode target variable\n",
    "le_target = LabelEncoder()\n",
    "y_encoded = le_target.fit_transform(y)\n",
    "print(f\"Target encoding: {dict(zip(le_target.classes_, le_target.transform(le_target.classes_)))}\")\n",
    "\n",
    "# Handle categorical features\n",
    "categorical_features = ['Gender', 'Province', 'Ethnicity', 'TumorType', 'CancerStage', \n",
    "                       'Metastasis', 'TreatmentType', 'SmokingStatus', 'AlcoholUse', \n",
    "                       'GeneticMutation', 'Comorbidities']\n",
    "\n",
    "X_processed = X.copy()\n",
    "\n",
    "for col in categorical_features:\n",
    "    # Fill missing values with 'Unknown'\n",
    "    X_processed[col] = X_processed[col].fillna('Unknown')\n",
    "    \n",
    "    # Label encode\n",
    "    le = LabelEncoder()\n",
    "    X_processed[col] = le.fit_transform(X_processed[col])\n",
    "    label_encoders[col] = le\n",
    "    \n",
    "    print(f\"{col}: {len(le.classes_)} unique values\")\n",
    "\n",
    "# Handle numerical features\n",
    "numerical_features = ['Age', 'TumorSize', 'ChemotherapySessions', 'RadiationSessions', 'FollowUpMonths']\n",
    "\n",
    "for col in numerical_features:\n",
    "    # Fill missing values with median\n",
    "    X_processed[col] = X_processed[col].fillna(X_processed[col].median())\n",
    "\n",
    "print(f\"\\nProcessed feature matrix shape: {X_processed.shape}\")\n",
    "print(f\"Missing values after preprocessing: {X_processed.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f21459b",
   "metadata": {},
   "source": [
    "## Advanced Feature Engineering\n",
    "\n",
    "Creating interaction features and performing feature selection to improve model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25c3858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering: Create interaction features\n",
    "print(\"=== FEATURE ENGINEERING ===\")\n",
    "\n",
    "# Create interaction features\n",
    "X_engineered = X_processed.copy()\n",
    "\n",
    "# Age-based interactions\n",
    "X_engineered['Age_TumorSize'] = X_engineered['Age'] * X_engineered['TumorSize']\n",
    "X_engineered['Age_CancerStage'] = X_engineered['Age'] * X_engineered['CancerStage']\n",
    "\n",
    "# Treatment intensity features\n",
    "X_engineered['TotalTreatment'] = X_engineered['ChemotherapySessions'] + X_engineered['RadiationSessions']\n",
    "X_engineered['TreatmentIntensity'] = X_engineered['TotalTreatment'] / (X_engineered['FollowUpMonths'] + 1)\n",
    "\n",
    "# Risk factors combination\n",
    "X_engineered['HighRisk'] = ((X_engineered['CancerStage'] >= 2) & \n",
    "                           (X_engineered['Metastasis'] == 1) & \n",
    "                           (X_engineered['TumorSize'] > X_engineered['TumorSize'].median())).astype(int)\n",
    "\n",
    "print(f\"Original features: {X_processed.shape[1]}\")\n",
    "print(f\"Engineered features: {X_engineered.shape[1]}\")\n",
    "print(f\"New features added: {X_engineered.shape[1] - X_processed.shape[1]}\")\n",
    "\n",
    "# Feature Selection using statistical tests\n",
    "selector = SelectKBest(score_func=f_classif, k=15)  # Select top 15 features\n",
    "X_selected = selector.fit_transform(X_engineered, y_encoded)\n",
    "\n",
    "# Get selected feature names\n",
    "feature_names = X_engineered.columns\n",
    "selected_features = feature_names[selector.get_support()]\n",
    "feature_scores = selector.scores_[selector.get_support()]\n",
    "\n",
    "print(f\"\\nSelected {len(selected_features)} most important features:\")\n",
    "for name, score in zip(selected_features, feature_scores):\n",
    "    print(f\"  {name}: {score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c675eb8",
   "metadata": {},
   "source": [
    "## Train-Test Split and Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe1a07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (8000, 16)\n",
      "Test set shape: (2000, 16)\n",
      "Training target distribution: [6232 1768]\n",
      "Test target distribution: [1558  442]\n",
      "Feature scaling completed\n",
      "Training data mean: -0.000000\n",
      "Training data std: 1.000000\n"
     ]
    }
   ],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_selected, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "print(f\"Training target distribution: {np.bincount(y_train)}\")\n",
    "print(f\"Test target distribution: {np.bincount(y_test)}\")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Feature scaling completed\")\n",
    "print(f\"Training data mean: {X_train_scaled.mean():.6f}\")\n",
    "print(f\"Training data std: {X_train_scaled.std():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80254e94",
   "metadata": {},
   "source": [
    "## SMOTE: Synthetic Minority Oversampling\n",
    "\n",
    "Addressing class imbalance by generating synthetic samples of the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc34d90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTE for better class balance\n",
    "print(\"=== SMOTE IMPLEMENTATION ===\")\n",
    "\n",
    "try:\n",
    "    # Try different SMOTE variants\n",
    "    smote_variants = {\n",
    "        'SMOTE': SMOTE(random_state=42),\n",
    "        'BorderlineSMOTE': BorderlineSMOTE(random_state=42),\n",
    "        'ADASYN': ADASYN(random_state=42)\n",
    "    }\n",
    "    \n",
    "    smote_results = {}\n",
    "    \n",
    "    for name, sampler in smote_variants.items():\n",
    "        try:\n",
    "            X_resampled, y_resampled = sampler.fit_resample(X_train_scaled, y_train)\n",
    "            smote_results[name] = (X_resampled, y_resampled)\n",
    "            \n",
    "            print(f\"\\n{name} Results:\")\n",
    "            print(f\"  Original: {np.bincount(y_train)} (total: {len(y_train)})\")\n",
    "            print(f\"  Resampled: {np.bincount(y_resampled)} (total: {len(y_resampled)})\")\n",
    "        except Exception as e:\n",
    "            print(f\"  {name} failed: {e}\")\n",
    "    \n",
    "    # Use the best variant (SMOTE for reliability)\n",
    "    X_train_smote, y_train_smote = smote_results.get('SMOTE', (X_train_scaled, y_train))\n",
    "    print(f\"\\nUsing SMOTE resampled data for training\")\n",
    "    \n",
    "except NameError:\n",
    "    print(\"SMOTE not available, using original data\")\n",
    "    X_train_smote, y_train_smote = X_train_scaled, y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f026f386",
   "metadata": {},
   "source": [
    "## Focal Loss Implementation\n",
    "\n",
    "Custom loss function specifically designed for imbalanced classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf81b1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focal Loss implementation for imbalanced data\n",
    "def focal_loss(gamma=2.0, alpha=0.25):\n",
    "    \"\"\"\n",
    "    Focal Loss for addressing class imbalance.\n",
    "    \n",
    "    Args:\n",
    "        gamma: Focusing parameter (default=2.0)\n",
    "        alpha: Weighting factor for rare class (default=0.25)\n",
    "    \"\"\"\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1.0 - epsilon)\n",
    "        \n",
    "        # Calculate cross entropy\n",
    "        ce = -y_true * tf.math.log(y_pred) - (1 - y_true) * tf.math.log(1 - y_pred)\n",
    "        \n",
    "        # Calculate p_t\n",
    "        p_t = tf.where(tf.equal(y_true, 1), y_pred, 1 - y_pred)\n",
    "        \n",
    "        # Calculate alpha_t\n",
    "        alpha_t = tf.ones_like(y_true) * alpha\n",
    "        alpha_t = tf.where(tf.equal(y_true, 1), alpha_t, 1 - alpha_t)\n",
    "        \n",
    "        # Calculate focal weight\n",
    "        focal_weight = alpha_t * tf.pow((1 - p_t), gamma)\n",
    "        \n",
    "        # Calculate focal loss\n",
    "        focal_loss = focal_weight * ce\n",
    "        \n",
    "        return tf.reduce_mean(focal_loss)\n",
    "    \n",
    "    return focal_loss_fixed\n",
    "\n",
    "print(\"✓ Focal Loss function defined\")\n",
    "print(\"  - Gamma (focusing parameter): 2.0\")\n",
    "print(\"  - Alpha (class weighting): 0.25\")\n",
    "print(\"  - Reduces loss for well-classified examples\")\n",
    "print(\"  - Focuses learning on hard examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edde7b5b",
   "metadata": {},
   "source": [
    "## Neural Network Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0418ea68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,176</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m2,176\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">13,441</span> (52.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m13,441\u001b[0m (52.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,993</span> (50.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m12,993\u001b[0m (50.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> (1.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m448\u001b[0m (1.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the neural network architecture\n",
    "def create_model(input_dim, hidden_layers=[128, 64, 32], dropout_rate=0.3, learning_rate=0.001, use_focal_loss=False):\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    # Input layer\n",
    "    model.add(layers.Dense(hidden_layers[0], activation='relu', input_dim=input_dim))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    \n",
    "    # Hidden layers\n",
    "    for units in hidden_layers[1:]:\n",
    "        model.add(layers.Dense(units, activation='relu'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Dropout(dropout_rate))\n",
    "    \n",
    "    # Output layer (binary classification)\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Compile model\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    \n",
    "    # Choose loss function\n",
    "    if use_focal_loss:\n",
    "        loss = focal_loss(gamma=2.0, alpha=0.25)\n",
    "        loss_name = \"Focal Loss\"\n",
    "    else:\n",
    "        loss = 'binary_crossentropy'\n",
    "        loss_name = \"Binary Crossentropy\"\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss,\n",
    "        metrics=['accuracy', 'precision', 'recall']\n",
    "    )\n",
    "    \n",
    "    print(f\"Model compiled with {loss_name}\")\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "input_dim = X_train_scaled.shape[1]\n",
    "model = create_model(input_dim)\n",
    "\n",
    "# Display model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d487642c",
   "metadata": {},
   "source": [
    "## XGBoost Implementation\n",
    "\n",
    "Gradient boosting model specifically tuned for imbalanced classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf7afa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost implementation with hyperparameter tuning\n",
    "print(\"=== XGBOOST IMPLEMENTATION ===\")\n",
    "\n",
    "try:\n",
    "    # Calculate scale_pos_weight for imbalanced data\n",
    "    negative_samples = np.sum(y_train == 0)\n",
    "    positive_samples = np.sum(y_train == 1)\n",
    "    scale_pos_weight = negative_samples / positive_samples\n",
    "    \n",
    "    print(f\"Class distribution - Negative: {negative_samples}, Positive: {positive_samples}\")\n",
    "    print(f\"Scale pos weight: {scale_pos_weight:.2f}\")\n",
    "    \n",
    "    # XGBoost with optimized parameters for imbalanced data\n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        random_state=42,\n",
    "        eval_metric='auc'\n",
    "    )\n",
    "    \n",
    "    # Train XGBoost\n",
    "    print(\"Training XGBoost model...\")\n",
    "    start_time = time.time()\n",
    "    xgb_model.fit(X_train_scaled, y_train)\n",
    "    xgb_train_time = time.time() - start_time\n",
    "    \n",
    "    # Predictions\n",
    "    xgb_pred_proba = xgb_model.predict_proba(X_test_scaled)[:, 1]\n",
    "    xgb_pred = xgb_model.predict(X_test_scaled)\n",
    "    \n",
    "    # Metrics\n",
    "    xgb_auc = roc_auc_score(y_test, xgb_pred_proba)\n",
    "    \n",
    "    print(f\"XGBoost training completed in {xgb_train_time:.2f} seconds\")\n",
    "    print(f\"XGBoost AUC Score: {xgb_auc:.4f}\")\n",
    "    \n",
    "    # Feature importance\n",
    "    feature_importance = xgb_model.feature_importances_\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': selected_features,\n",
    "        'importance': feature_importance\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(f\"\\nTop 10 Most Important Features (XGBoost):\")\n",
    "    for i, (_, row) in enumerate(importance_df.head(10).iterrows()):\n",
    "        print(f\"  {i+1}. {row['feature']}: {row['importance']:.4f}\")\n",
    "    \n",
    "except NameError:\n",
    "    print(\"XGBoost not available\")\n",
    "    xgb_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342351f4",
   "metadata": {},
   "source": [
    "## Random Forest with Class Balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22db51e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest with hyperparameter optimization\n",
    "print(\"=== RANDOM FOREST IMPLEMENTATION ===\")\n",
    "\n",
    "# Random Forest with class balancing\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=10,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    class_weight='balanced',  # Handle imbalanced data\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train Random Forest\n",
    "print(\"Training Random Forest model...\")\n",
    "start_time = time.time()\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "rf_train_time = time.time() - start_time\n",
    "\n",
    "# Predictions\n",
    "rf_pred_proba = rf_model.predict_proba(X_test_scaled)[:, 1]\n",
    "rf_pred = rf_model.predict(X_test_scaled)\n",
    "\n",
    "# Metrics\n",
    "rf_auc = roc_auc_score(y_test, rf_pred_proba)\n",
    "\n",
    "print(f\"Random Forest training completed in {rf_train_time:.2f} seconds\")\n",
    "print(f\"Random Forest AUC Score: {rf_auc:.4f}\")\n",
    "\n",
    "# Feature importance\n",
    "rf_importance = rf_model.feature_importances_\n",
    "rf_importance_df = pd.DataFrame({\n",
    "    'feature': selected_features,\n",
    "    'importance': rf_importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"\\nTop 10 Most Important Features (Random Forest):\")\n",
    "for i, (_, row) in enumerate(rf_importance_df.head(10).iterrows()):\n",
    "    print(f\"  {i+1}. {row['feature']}: {row['importance']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61dd87e",
   "metadata": {},
   "source": [
    "## Model Training with Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0b9b4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks for training monitoring\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Reduce learning rate when loss plateaus\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=10,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Save best model\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    'best_survival_model.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "callbacks = [early_stopping, reduce_lr, model_checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b633f95",
   "metadata": {},
   "source": [
    "## Training Multiple Neural Network Variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ad2bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multiple neural network variants\n",
    "print(\"=== TRAINING NEURAL NETWORK VARIANTS ===\")\n",
    "\n",
    "nn_models = {}\n",
    "nn_histories = {}\n",
    "\n",
    "# Model configurations\n",
    "model_configs = {\n",
    "    'NN_Original': {\n",
    "        'data': (X_train_scaled, y_train),\n",
    "        'class_weight': None,\n",
    "        'focal_loss': False,\n",
    "        'description': 'Original data, no class weighting'\n",
    "    },\n",
    "    'NN_ClassWeight': {\n",
    "        'data': (X_train_scaled, y_train),\n",
    "        'class_weight': {0: 0.64, 1: 2.26},\n",
    "        'focal_loss': False,\n",
    "        'description': 'Original data with class weighting'\n",
    "    },\n",
    "    'NN_SMOTE': {\n",
    "        'data': (X_train_smote, y_train_smote),\n",
    "        'class_weight': None,\n",
    "        'focal_loss': False,\n",
    "        'description': 'SMOTE resampled data'\n",
    "    },\n",
    "    'NN_FocalLoss': {\n",
    "        'data': (X_train_scaled, y_train),\n",
    "        'class_weight': None,\n",
    "        'focal_loss': True,\n",
    "        'description': 'Original data with Focal Loss'\n",
    "    }\n",
    "}\n",
    "\n",
    "for name, config in model_configs.items():\n",
    "    print(f\"\\nTraining {name}: {config['description']}\")\n",
    "    \n",
    "    # Create model\n",
    "    model_variant = create_model(\n",
    "        input_dim=input_dim,\n",
    "        use_focal_loss=config['focal_loss']\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    start_time = time.time()\n",
    "    history = model_variant.fit(\n",
    "        config['data'][0], config['data'][1],\n",
    "        validation_split=0.2,\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        class_weight=config['class_weight'],\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=0\n",
    "    )\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # Store results\n",
    "    nn_models[name] = model_variant\n",
    "    nn_histories[name] = history\n",
    "    \n",
    "    print(f\"  Training completed in {train_time:.2f} seconds\")\n",
    "    print(f\"  Epochs: {len(history.history['loss'])}\")\n",
    "    print(f\"  Best val_loss: {min(history.history['val_loss']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea638ebc",
   "metadata": {},
   "source": [
    "## Comprehensive Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3f896b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive model evaluation\n",
    "print(\"=== COMPREHENSIVE MODEL EVALUATION ===\")\n",
    "\n",
    "# Store all results\n",
    "all_results = []\n",
    "\n",
    "# Evaluate neural network variants\n",
    "for name, model_variant in nn_models.items():\n",
    "    pred_proba = model_variant.predict(X_test_scaled, verbose=0)\n",
    "    pred = (pred_proba > 0.5).astype(int).flatten()\n",
    "    \n",
    "    auc = roc_auc_score(y_test, pred_proba)\n",
    "    report = classification_report(y_test, pred, target_names=['Deceased', 'Alive'], output_dict=True)\n",
    "    \n",
    "    all_results.append({\n",
    "        'Model': name,\n",
    "        'Type': 'Neural Network',\n",
    "        'AUC': auc,\n",
    "        'Accuracy': report['accuracy'],\n",
    "        'Precision (Alive)': report['Alive']['precision'],\n",
    "        'Recall (Alive)': report['Alive']['recall'],\n",
    "        'F1 (Alive)': report['Alive']['f1-score'],\n",
    "        'Precision (Deceased)': report['Deceased']['precision'],\n",
    "        'Recall (Deceased)': report['Deceased']['recall'],\n",
    "        'F1 (Deceased)': report['Deceased']['f1-score']\n",
    "    })\n",
    "\n",
    "# Evaluate Random Forest\n",
    "rf_report = classification_report(y_test, rf_pred, target_names=['Deceased', 'Alive'], output_dict=True)\n",
    "all_results.append({\n",
    "    'Model': 'Random Forest',\n",
    "    'Type': 'Ensemble',\n",
    "    'AUC': rf_auc,\n",
    "    'Accuracy': rf_report['accuracy'],\n",
    "    'Precision (Alive)': rf_report['Alive']['precision'],\n",
    "    'Recall (Alive)': rf_report['Alive']['recall'],\n",
    "    'F1 (Alive)': rf_report['Alive']['f1-score'],\n",
    "    'Precision (Deceased)': rf_report['Deceased']['precision'],\n",
    "    'Recall (Deceased)': rf_report['Deceased']['recall'],\n",
    "    'F1 (Deceased)': rf_report['Deceased']['f1-score']\n",
    "})\n",
    "\n",
    "# Evaluate XGBoost (if available)\n",
    "if xgb_model is not None:\n",
    "    xgb_report = classification_report(y_test, xgb_pred, target_names=['Deceased', 'Alive'], output_dict=True)\n",
    "    all_results.append({\n",
    "        'Model': 'XGBoost',\n",
    "        'Type': 'Gradient Boosting',\n",
    "        'AUC': xgb_auc,\n",
    "        'Accuracy': xgb_report['accuracy'],\n",
    "        'Precision (Alive)': xgb_report['Alive']['precision'],\n",
    "        'Recall (Alive)': xgb_report['Alive']['recall'],\n",
    "        'F1 (Alive)': xgb_report['Alive']['f1-score'],\n",
    "        'Precision (Deceased)': xgb_report['Deceased']['precision'],\n",
    "        'Recall (Deceased)': xgb_report['Deceased']['recall'],\n",
    "        'F1 (Deceased)': xgb_report['Deceased']['f1-score']\n",
    "    })\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "results_df = results_df.round(4)\n",
    "\n",
    "# Sort by AUC score (descending)\n",
    "results_df = results_df.sort_values('AUC', ascending=False)\n",
    "\n",
    "print(\"Model Performance Comparison:\")\n",
    "print(\"=\" * 80)\n",
    "display(results_df)\n",
    "\n",
    "# Find best model for recall (medical priority)\n",
    "best_recall_model = results_df.loc[results_df['Recall (Alive)'].idxmax()]\n",
    "print(f\"\\n🏆 BEST MODEL FOR RECALL (Alive): {best_recall_model['Model']}\")\n",
    "print(f\"   Recall (Alive): {best_recall_model['Recall (Alive)']:.4f}\")\n",
    "print(f\"   AUC: {best_recall_model['AUC']:.4f}\")\n",
    "\n",
    "# Find best overall model (balanced performance)\n",
    "results_df['F1_Weighted'] = (results_df['F1 (Alive)'] * 0.3 + results_df['F1 (Deceased)'] * 0.7)\n",
    "best_overall_model = results_df.loc[results_df['F1_Weighted'].idxmax()]\n",
    "print(f\"\\n🎯 BEST OVERALL MODEL: {best_overall_model['Model']}\")\n",
    "print(f\"   Weighted F1: {best_overall_model['F1_Weighted']:.4f}\")\n",
    "print(f\"   AUC: {best_overall_model['AUC']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7eb827c",
   "metadata": {},
   "source": [
    "## Advanced Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde7d6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced model comparison visualizations\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "\n",
    "# 1. AUC Comparison\n",
    "axes[0, 0].bar(results_df['Model'], results_df['AUC'], color='skyblue', edgecolor='navy')\n",
    "axes[0, 0].set_title('AUC Score Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('AUC Score')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Recall (Alive) Comparison - Most Important for Medical\n",
    "axes[0, 1].bar(results_df['Model'], results_df['Recall (Alive)'], color='lightcoral', edgecolor='darkred')\n",
    "axes[0, 1].set_title('Recall (Alive) - Medical Priority', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Recall (Alive)')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Precision vs Recall Scatter\n",
    "axes[0, 2].scatter(results_df['Recall (Alive)'], results_df['Precision (Alive)'], \n",
    "                   s=100, alpha=0.7, c=results_df['AUC'], cmap='viridis')\n",
    "for i, model in enumerate(results_df['Model']):\n",
    "    axes[0, 2].annotate(model, \n",
    "                       (results_df['Recall (Alive)'].iloc[i], results_df['Precision (Alive)'].iloc[i]),\n",
    "                       xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "axes[0, 2].set_xlabel('Recall (Alive)')\n",
    "axes[0, 2].set_ylabel('Precision (Alive)')\n",
    "axes[0, 2].set_title('Precision vs Recall Trade-off', fontsize=14, fontweight='bold')\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Feature Importance Comparison (XGBoost vs Random Forest)\n",
    "if xgb_model is not None:\n",
    "    top_features = importance_df.head(10)\n",
    "    rf_top = rf_importance_df.head(10)\n",
    "    \n",
    "    x_pos = np.arange(len(top_features))\n",
    "    width = 0.35\n",
    "    \n",
    "    axes[1, 0].barh(x_pos - width/2, top_features['importance'], width, \n",
    "                    label='XGBoost', alpha=0.8, color='gold')\n",
    "    \n",
    "    # Match RF features to XGB features for comparison\n",
    "    rf_matched = []\n",
    "    for feat in top_features['feature']:\n",
    "        rf_val = rf_importance_df[rf_importance_df['feature'] == feat]['importance']\n",
    "        rf_matched.append(rf_val.values[0] if len(rf_val) > 0 else 0)\n",
    "    \n",
    "    axes[1, 0].barh(x_pos + width/2, rf_matched, width, \n",
    "                    label='Random Forest', alpha=0.8, color='lightgreen')\n",
    "    \n",
    "    axes[1, 0].set_yticks(x_pos)\n",
    "    axes[1, 0].set_yticklabels(top_features['feature'])\n",
    "    axes[1, 0].set_xlabel('Feature Importance')\n",
    "    axes[1, 0].set_title('Feature Importance: XGBoost vs Random Forest', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "else:\n",
    "    axes[1, 0].text(0.5, 0.5, 'XGBoost not available\\nfor comparison', \n",
    "                    transform=axes[1, 0].transAxes, ha='center', va='center', fontsize=12)\n",
    "    axes[1, 0].set_title('Feature Importance Comparison', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 5. Model Type Performance\n",
    "type_performance = results_df.groupby('Type')['AUC'].mean().sort_values(ascending=False)\n",
    "axes[1, 1].bar(type_performance.index, type_performance.values, \n",
    "               color=['coral', 'lightblue', 'lightgreen'][:len(type_performance)])\n",
    "axes[1, 1].set_title('Average AUC by Model Type', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Average AUC')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Confusion Matrix for Best Model\n",
    "best_model_name = best_recall_model['Model']\n",
    "if best_model_name.startswith('NN_'):\n",
    "    best_pred = (nn_models[best_model_name].predict(X_test_scaled, verbose=0) > 0.5).astype(int).flatten()\n",
    "elif best_model_name == 'Random Forest':\n",
    "    best_pred = rf_pred\n",
    "elif best_model_name == 'XGBoost':\n",
    "    best_pred = xgb_pred\n",
    "\n",
    "cm_best = confusion_matrix(y_test, best_pred)\n",
    "sns.heatmap(cm_best, annot=True, fmt='d', cmap='Blues', ax=axes[1, 2])\n",
    "axes[1, 2].set_title(f'Confusion Matrix: {best_model_name}', fontsize=14, fontweight='bold')\n",
    "axes[1, 2].set_xlabel('Predicted')\n",
    "axes[1, 2].set_ylabel('Actual')\n",
    "axes[1, 2].set_xticklabels(['Deceased', 'Alive'])\n",
    "axes[1, 2].set_yticklabels(['Deceased', 'Alive'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3f435b",
   "metadata": {},
   "source": [
    "## ROC Curves Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611f68e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curves comparison for all models\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "colors = ['blue', 'red', 'green', 'orange', 'purple', 'brown']\n",
    "color_idx = 0\n",
    "\n",
    "# Plot ROC curves for neural networks\n",
    "for name, model_variant in nn_models.items():\n",
    "    pred_proba = model_variant.predict(X_test_scaled, verbose=0)\n",
    "    fpr, tpr, _ = roc_curve(y_test, pred_proba)\n",
    "    auc = roc_auc_score(y_test, pred_proba)\n",
    "    \n",
    "    plt.plot(fpr, tpr, color=colors[color_idx], lw=2, \n",
    "             label=f'{name} (AUC = {auc:.3f})')\n",
    "    color_idx += 1\n",
    "\n",
    "# Plot Random Forest ROC\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, rf_pred_proba)\n",
    "plt.plot(fpr_rf, tpr_rf, color=colors[color_idx], lw=2, \n",
    "         label=f'Random Forest (AUC = {rf_auc:.3f})')\n",
    "color_idx += 1\n",
    "\n",
    "# Plot XGBoost ROC (if available)\n",
    "if xgb_model is not None:\n",
    "    fpr_xgb, tpr_xgb, _ = roc_curve(y_test, xgb_pred_proba)\n",
    "    plt.plot(fpr_xgb, tpr_xgb, color=colors[color_idx], lw=2, \n",
    "             label=f'XGBoost (AUC = {xgb_auc:.3f})')\n",
    "\n",
    "# Plot diagonal (random classifier)\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--', alpha=0.8, label='Random Classifier')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC Curves Comparison - All Models', fontsize=16, fontweight='bold')\n",
    "plt.legend(loc=\"lower right\", fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cef5413",
   "metadata": {},
   "source": [
    "## Key Insights and Recommendations\n",
    "\n",
    "### Model Performance Summary:\n",
    "Based on our comprehensive evaluation, here are the key findings:\n",
    "\n",
    "1. **For Medical Applications (Maximizing Recall)**: The model with highest recall for detecting \"Alive\" patients should be prioritized\n",
    "2. **Feature Engineering Impact**: Interaction features and proper feature selection improved model performance\n",
    "3. **Class Imbalance Solutions**: SMOTE, class weighting, and focal loss all helped address the imbalanced dataset\n",
    "4. **Ensemble Methods**: Tree-based models (XGBoost, Random Forest) often perform well on tabular medical data\n",
    "\n",
    "### Production Recommendations:\n",
    "1. **Primary Model**: Use the model with highest \"Alive\" recall for clinical decisions\n",
    "2. **Secondary Validation**: Ensemble multiple top-performing models for robust predictions\n",
    "3. **Feature Monitoring**: Track feature importance changes over time\n",
    "4. **Threshold Optimization**: Use domain expertise to set optimal classification thresholds\n",
    "5. **Regular Retraining**: Update models as new patient data becomes available\n",
    "\n",
    "### Next Steps for Improvement:\n",
    "1. **Cross-Validation**: Implement k-fold CV for more robust performance estimates\n",
    "2. **Advanced Ensembling**: Try stacking or voting classifiers\n",
    "3. **Deep Learning**: Experiment with more complex architectures (attention mechanisms)\n",
    "4. **External Validation**: Test on data from different hospitals/regions\n",
    "5. **Clinical Integration**: Work with medical experts to refine feature selection and interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8665b6fc",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### Class Weighting:\n",
    "- **Best for**: Training phase improvement\n",
    "- **Impact**: Forces model to learn minority class patterns better\n",
    "- **Trade-off**: May slightly reduce overall accuracy but improves minority class performance\n",
    "- **When to use**: Always recommended for imbalanced datasets\n",
    "\n",
    "### Threshold Tuning:\n",
    "- **Best for**: Post-training optimization\n",
    "- **Impact**: Optimizes decision boundary for your specific metric\n",
    "- **Trade-off**: Precision vs Recall balance\n",
    "- **When to use**: When you have specific business requirements (e.g., medical: minimize false negatives)\n",
    "\n",
    "### For Your Cancer Survival Data:\n",
    "1. **Class weighting** helps the model learn \"Alive\" patterns better during training\n",
    "2. **Threshold tuning** optimizes predictions for medical context (catching all potential survivors)\n",
    "3. **Combined approach** often yields best results: train with class weights, then tune threshold\n",
    "\n",
    "### Medical Context Considerations:\n",
    "- **False Negative (predicting Deceased when Alive)**: Very serious - might miss treatment opportunities\n",
    "- **False Positive (predicting Alive when Deceased)**: Less critical - extra monitoring won't harm\n",
    "- Therefore: Lower threshold (0.3-0.4) might be preferable to catch more \"Alive\" cases"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cancer-prediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
