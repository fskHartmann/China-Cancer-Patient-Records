{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9895b8e",
   "metadata": {},
   "source": [
    "# Cancer Patient Survival Prediction using Neural Networks\n",
    "\n",
    "This notebook implements a feedforward neural network to predict patient survival status based on clinical features from the China Cancer Patient Records dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c003b8e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPU available:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe39a4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data_path = \"/Users/f/.cache/kagglehub/datasets/ak0212/china-cancer-patient-records/versions/1/china_cancer_patients_synthetic.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Target distribution:\\n{df['SurvivalStatus'].value_counts()}\")\n",
    "print(f\"Target distribution (%):\\n{df['SurvivalStatus'].value_counts(normalize=True) * 100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b38a284",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5a50e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for preprocessing\n",
    "data = df.copy()\n",
    "\n",
    "# Check missing values\n",
    "print(\"Missing values per column:\")\n",
    "print(data.isnull().sum())\n",
    "print(\"\\nMissing values percentage:\")\n",
    "print((data.isnull().sum() / len(data)) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c8a85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for the model (excluding ID, dates, and target)\n",
    "feature_columns = ['Gender', 'Age', 'Province', 'Ethnicity', 'TumorType', 'CancerStage', \n",
    "                  'TumorSize', 'Metastasis', 'TreatmentType', 'ChemotherapySessions', \n",
    "                  'RadiationSessions', 'FollowUpMonths', 'SmokingStatus', 'AlcoholUse', \n",
    "                  'GeneticMutation', 'Comorbidities']\n",
    "\n",
    "# Create feature dataframe\n",
    "X = data[feature_columns].copy()\n",
    "y = data['SurvivalStatus'].copy()\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Target vector shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc16c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values and encode categorical variables\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize label encoders\n",
    "label_encoders = {}\n",
    "\n",
    "# Encode target variable\n",
    "le_target = LabelEncoder()\n",
    "y_encoded = le_target.fit_transform(y)\n",
    "print(f\"Target encoding: {dict(zip(le_target.classes_, le_target.transform(le_target.classes_)))}\")\n",
    "\n",
    "# Handle categorical features\n",
    "categorical_features = ['Gender', 'Province', 'Ethnicity', 'TumorType', 'CancerStage', \n",
    "                       'Metastasis', 'TreatmentType', 'SmokingStatus', 'AlcoholUse', \n",
    "                       'GeneticMutation', 'Comorbidities']\n",
    "\n",
    "X_processed = X.copy()\n",
    "\n",
    "for col in categorical_features:\n",
    "    # Fill missing values with 'Unknown'\n",
    "    X_processed[col] = X_processed[col].fillna('Unknown')\n",
    "    \n",
    "    # Label encode\n",
    "    le = LabelEncoder()\n",
    "    X_processed[col] = le.fit_transform(X_processed[col])\n",
    "    label_encoders[col] = le\n",
    "    \n",
    "    print(f\"{col}: {len(le.classes_)} unique values\")\n",
    "\n",
    "# Handle numerical features\n",
    "numerical_features = ['Age', 'TumorSize', 'ChemotherapySessions', 'RadiationSessions', 'FollowUpMonths']\n",
    "\n",
    "for col in numerical_features:\n",
    "    # Fill missing values with median\n",
    "    X_processed[col] = X_processed[col].fillna(X_processed[col].median())\n",
    "\n",
    "print(f\"\\nProcessed feature matrix shape: {X_processed.shape}\")\n",
    "print(f\"Missing values after preprocessing: {X_processed.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c675eb8",
   "metadata": {},
   "source": [
    "## Train-Test Split and Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe1a07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_processed, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "print(f\"Training target distribution: {np.bincount(y_train)}\")\n",
    "print(f\"Test target distribution: {np.bincount(y_test)}\")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Feature scaling completed\")\n",
    "print(f\"Training data mean: {X_train_scaled.mean():.6f}\")\n",
    "print(f\"Training data std: {X_train_scaled.std():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edde7b5b",
   "metadata": {},
   "source": [
    "## Neural Network Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0418ea68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network architecture\n",
    "def create_model(input_dim, hidden_layers=[128, 64, 32], dropout_rate=0.3, learning_rate=0.001):\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    # Input layer\n",
    "    model.add(layers.Dense(hidden_layers[0], activation='relu', input_dim=input_dim))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    \n",
    "    # Hidden layers\n",
    "    for units in hidden_layers[1:]:\n",
    "        model.add(layers.Dense(units, activation='relu'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Dropout(dropout_rate))\n",
    "    \n",
    "    # Output layer (binary classification)\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Compile model\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', 'precision', 'recall']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "input_dim = X_train_scaled.shape[1]\n",
    "model = create_model(input_dim)\n",
    "\n",
    "# Display model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61dd87e",
   "metadata": {},
   "source": [
    "## Model Training with Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b9b4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks for training monitoring\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Reduce learning rate when loss plateaus\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=10,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Save best model\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    'best_survival_model.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "callbacks = [early_stopping, reduce_lr, model_checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3af51d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with comprehensive tracking\n",
    "print(\"Starting model training...\")\n",
    "start_time = time.time()\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "\n",
    "print(f\"\\nTraining completed!\")\n",
    "print(f\"Total training time: {training_time:.2f} seconds ({training_time/60:.2f} minutes)\")\n",
    "print(f\"Number of epochs completed: {len(history.history['loss'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63868a8",
   "metadata": {},
   "source": [
    "## Training Progress Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2e8f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Loss\n",
    "axes[0, 0].plot(history.history['loss'], label='Training Loss')\n",
    "axes[0, 0].plot(history.history['val_loss'], label='Validation Loss')\n",
    "axes[0, 0].set_title('Model Loss')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# Accuracy\n",
    "axes[0, 1].plot(history.history['accuracy'], label='Training Accuracy')\n",
    "axes[0, 1].plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "axes[0, 1].set_title('Model Accuracy')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Accuracy')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Precision\n",
    "axes[1, 0].plot(history.history['precision'], label='Training Precision')\n",
    "axes[1, 0].plot(history.history['val_precision'], label='Validation Precision')\n",
    "axes[1, 0].set_title('Model Precision')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Precision')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Recall\n",
    "axes[1, 1].plot(history.history['recall'], label='Training Recall')\n",
    "axes[1, 1].plot(history.history['val_recall'], label='Validation Recall')\n",
    "axes[1, 1].set_title('Model Recall')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Recall')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f411ee",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53d7886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "print(\"Evaluating model on test set...\")\n",
    "y_pred_proba = model.predict(X_test_scaled)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n",
    "\n",
    "# Calculate metrics\n",
    "test_loss, test_accuracy, test_precision, test_recall = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(f\"\\n=== Model Performance ===\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test Precision: {test_precision:.4f}\")\n",
    "print(f\"Test Recall: {test_recall:.4f}\")\n",
    "print(f\"AUC Score: {auc_score:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(f\"\\n=== Classification Report ===\")\n",
    "target_names = ['Deceased', 'Alive']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01332c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix and ROC Curve\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0])\n",
    "axes[0].set_title('Confusion Matrix')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "axes[0].set_xticklabels(['Deceased', 'Alive'])\n",
    "axes[0].set_yticklabels(['Deceased', 'Alive'])\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "axes[1].plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {auc_score:.2f})')\n",
    "axes[1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "axes[1].set_xlim([0.0, 1.0])\n",
    "axes[1].set_ylim([0.0, 1.05])\n",
    "axes[1].set_xlabel('False Positive Rate')\n",
    "axes[1].set_ylabel('True Positive Rate')\n",
    "axes[1].set_title('Receiver Operating Characteristic (ROC) Curve')\n",
    "axes[1].legend(loc=\"lower right\")\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9f575a",
   "metadata": {},
   "source": [
    "## Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7469030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final training summary\n",
    "print(\"=== TRAINING SUMMARY ===\")\n",
    "print(f\"Dataset size: {len(df):,} patients\")\n",
    "print(f\"Training set: {len(X_train):,} patients\")\n",
    "print(f\"Test set: {len(X_test):,} patients\")\n",
    "print(f\"Number of features: {input_dim}\")\n",
    "print(f\"Training time: {training_time:.2f} seconds ({training_time/60:.2f} minutes)\")\n",
    "print(f\"Epochs completed: {len(history.history['loss'])}\")\n",
    "print(f\"Best validation loss: {min(history.history['val_loss']):.4f}\")\n",
    "print(f\"Best validation accuracy: {max(history.history['val_accuracy']):.4f}\")\n",
    "print(f\"Final test accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Final AUC score: {auc_score:.4f}\")\n",
    "\n",
    "# Model architecture summary\n",
    "print(f\"\\n=== MODEL ARCHITECTURE ===\")\n",
    "total_params = model.count_params()\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Model layers: {len(model.layers)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myproj)",
   "language": "python",
   "name": "myproj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
